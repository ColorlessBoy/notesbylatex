\documentclass[a4paper]{article}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{url}

\newtheorem{defn}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}[section]
\numberwithin{figure}{section}
\numberwithin{equation}{section}

\title{MDP:Preliminaries}
\author{Peng Lingwei}
\begin{document}

\maketitle
\tableofcontents

\section{Basic model}%
\label{sec:basic_model}

\begin{enumerate}
    \item State set: $ S = \{ s_1, s_2, \dots, s_n \} $ 
    \item Action set: $ A = \{ a_1, a_2, \dots, a_m \} $ 
    \item State transition matrix: 
        \[
            P = 
            \begin{pmatrix}
                \vec{s_{11}} & \vec{s_{12}} & \cdots & \vec{s_{1m}} \\
                \vec{s_{21}} & \vec{s_{22}} & \cdots & \vec{s_{2m}} \\
                \vdots       & \vdots       & \ddots & \vdots  \\
                \vec{s_{n1}} & \vec{s_{n2}} & \cdots & \vec{s_{nm}} \\
            \end{pmatrix}
        \]
    \item Reward matrix:
        \[
            R =
            \begin{pmatrix}
                r_{11} & r_{12} & \cdots & r_{1m} \\
                r_{21} & r_{22} & \cdots & r_{2m} \\
                \vdots & \vdots & \ddots & \vdots \\
                r_{n1} & r_{n2} & \cdots & r_{nm} \\
            \end{pmatrix}
        \]
    \item Decision matrix:
        \[
            d_t =
            \begin{pmatrix}
                d_{11} & d_{12} & \cdots & d_{1m} \\
                d_{21} & d_{22} & \cdots & d_{2m} \\
                \vdots & \vdots & \ddots & \vdots \\
                d_{n1} & d_{n2} & \cdots & d_{nm} \\
            \end{pmatrix}
        \]
    \item Policy process: $ \pi = \{ d_0, d_1, \ldots, d_t, \ldots \} $ 
    \item Matrix product:
        \[
            \left\langle 
                \begin{pmatrix}
                    \vec{a_1} \\
                    \vec{a_2} \\
                    \vdots \\
                    \vec{a_n}
                \end{pmatrix},
                \begin{pmatrix}
                    \vec{b_1} \\
                    \vec{b_2} \\
                    \vdots \\
                    \vec{b_n}
                \end{pmatrix}
            \right\rangle
            =
            \begin{pmatrix}
                \langle \vec{a_1}, \vec{b_1} \rangle \\
                \langle \vec{a_2}, \vec{b_2} \rangle \\
                \vdots \\
                \langle \vec{a_n}, \vec{b_n} \rangle
            \end{pmatrix}
        \]

    \item Process' state transition: 
        \begin{align*}
            P^{\pi} =& \{ P_0, P_1, \ldots, P_t, \ldots \} \\
            =& \{ \langle P, d_0 \rangle, \langle P, d_1 \rangle, \ldots, \langle P, d_t \rangle, \ldots \}
        \end{align*}
    \item Process' reward:
        \begin{align*}
            R^{\pi} =& \{ R_0, R_1, \ldots, R_t, \ldots \} \\
            =& \{ \langle R, d_0 \rangle, \langle R, d_1 \rangle, \ldots, \langle R, d_t \rangle, \ldots \}
        \end{align*}
    \item Process's value function:
        \begin{align*}
            V^{\pi} =& \mathbb{E} \left\{ \sum^{\infty}_{t=0} \alpha^t r(s_t, a_t, s_{t+1}) \right\} \\
            =& \sum^{\infty}_{t=0} \alpha^t \prod^{t-1}_{k=0}P_k R_t 
        \end{align*}
    \item Optimal process' value:
        $ V^* = \sup_{\pi} V $,
        corresponding policy $ \pi^* $ is called $ \alpha-optimal $ policy.
\end{enumerate}
    
\section{Basic theorem}%
\label{sec:basic_theorem}

\begin{thm}
    \[
        V^* = \max_a (R(\cdot, a) + \alpha P(\cdot, a) V^*)
    \]
    \[
        V^{\pi^* = \{ d^*_0, d^*_1, \ldots \}} 
        = \max_a (R(\cdot, a) + \alpha P(\cdot, a) V^{\pi^* = \{ d^*_0, d^*_1, \ldots \}} )
    \]
    \begin{proof}
        $ \forall \pi = \{ d_0, d_1, \ldots \} $, we have
        \begin{align*}
            V^{\pi = \{ d_0, d_1, \ldots \}} 
            =& R_0 + \alpha P_0 V^{\pi' = \{ d_1, d_2, \ldots \}} \\
            \preceq& R_0 + \alpha P_0 V^* \\
            \preceq& \max_a (R(\cdot, a) + \alpha P(\cdot, a) V^*) \\
            V^* \preceq& \max_a (R(\cdot, a) + \alpha P(\cdot, a) V^*) 
        \end{align*}
        Let $ a_0 = \max_a (R(\cdot, a) + \alpha P(\cdot, a) V^*) $, 
        and $ d = \mathbf{1} \{ a = a_0 \} $,
        so we construct that:
        $ \pi = \{ d, d_1, \ldots \} $,
        and $ \pi' = \{ d_1, d_2, \ldots \} $, then:
        \[
            \forall \epsilon, \exists \pi' = \{d_1, d_2, \ldots\}, \quad
            V^{\pi' = \{d_1, d_2, \ldots \}}
            \succeq V^* - \epsilon
        \]
        \begin{align*}
            V^* \succeq V^{\pi} =& R^*_0 + \alpha P^*_0 V^{\pi'} \\
            \succeq& R^*_0 + \alpha P^*_0 V^* - \alpha \epsilon 
        \end{align*}
        because $\epsilon $ is arbitrary,so
        \[
            V^* \succeq \max_a (R(\cdot, a) + \alpha P(\cdot, a) V^*) 
        \]
    \end{proof}
\end{thm}

\begin{thm}
    $ \exists \pi = \{ d, d, \ldots, \} $ is $ \alpha-optimal $ policy. 
    \begin{proof}
        Let $ d = \mathbf{1} \{ a = \max_{a'} (R(\cdot, a') + \alpha P(\cdot, a')V^*) \}$ 
        (This construction maybe problematic.)
        From preceeding theorem, we can get:
        \begin{align*}
            V^{\pi^* = \{ d^*_0, d^*_1, \ldots \}}
            =& \max_a (R(\cdot, a) + \alpha P(\cdot, a) V^{\pi^* = \{ d^*_1, d^*_2, \ldots \}} )\\
            =& \max_a (R(\cdot, a) + \alpha P(\cdot, a) V^{\pi^* = \{ d^*_0, d^*_1, \ldots \}} ) \\
            =& \langle R, d \rangle + \alpha \langle P, d \rangle V^{\pi^* = \{ d^*_0, d^*_1, \ldots \}} \\
            =& \sum^{n}_{t=0} \alpha^t \langle P, d \rangle^t \langle R, d \rangle
            + \alpha^n \langle P, d \rangle^n V^{\pi^* = \{ d^*_0, d^*_1, \ldots \}} \\
            n \rightarrow \infty,\quad V^* =& V^\pi
        \end{align*}
    \end{proof}
\end{thm}

\begin{thm}
    Let $ T_{\pi= \{ d, d, \ldots \} } V = \langle P, d \rangle + \alpha \langle P, d \rangle V $, 
    $ d $ and $ V $ are arbitrary, then
    \[
        n \rightarrow \infty, \quad T^n_{\pi} V = V^{\pi}
    \]
\end{thm}

\begin{thm}
    If $ T_{\pi_2} V^{\pi_1} = \max_a (R(\cdot, a) + \alpha P(\cdot, a) V^{\pi_1}) $, then $ V^{\pi_2} \succeq V^{\pi_1} $.
    \begin{proof}
        \[
             T_{\pi_2} V^{\pi_1} = \max_a (R(\cdot, a) + \alpha P(\cdot, a) V^{\pi_1}) 
            \succeq T_{\pi_1} V^{\pi_1} = V^{\pi_1}
        \]
        \[
            n \rightarrow \infty, \quad V^{\pi_2} = T^n_{\pi_2} V^{\pi_1} \succeq V^{\pi_1}
        \]
    \end{proof}
\end{thm}

\begin{thm}
    If $ U \succeq \max_a (R(\cdot, a) + \alpha P(\cdot, a) U) $, then $ U \succeq V^* $.
    \begin{proof}
        $ U \succeq V^*_n + \alpha^n \mathbb{E}^\pi [U(s_t) | s_0] \rightarrow V^*, as\ n \rightarrow \infty $ 
        (Using next section's proof, value Improvement).
    \end{proof}
\end{thm}

\begin{thm}
    The equation of $ V $ has unique solution.
    \[
        V = \max_a [ R(\cdot, a) + \alpha P(\cdot, a) V ]
    \]
    \begin{proof}
        Assuming that the equation has two solution $ U $, $ V $, then
        \begin{align*}
            U - V =& \max_a [ R(\cdot, a) + \alpha P(\cdot, a) U ] - \max_a [ R(\cdot, a) + \alpha P(\cdot, a) V ]\\
            =& [ R(\cdot, a_U) + \alpha P(\cdot, a_U) V ] - \max_a [ R(\cdot, a) + \alpha P(\cdot, a) V ]\\
            \preceq& \alpha P(\cdot, a_U) [U - V]\\
            \preceq& \alpha \sup |U - V| \cdot \vec{e}, \quad ( \vec{e} = {[1, 1, 1, \ldots, 1]}^T )
        \end{align*}
        Similarly,
        \[
            V - U \preceq& \alpha \sup |U - V| \cdot \vec{e}
        \]
        So, $ \sup |U - V| = 0 $, $ U = V $  
    \end{proof}
\end{thm}

\begin{thm}
    The equation of V about $ \pi = \{ d, d, \dots \} $ has unique solution, and the solution is $ V^\pi $.
    \[
        V = \langle R, d \rangle + \alpha \langle P, d \rangle V
    \]
\end{thm}

\begin{thm}
    
\end{thm}

\section{Value Improvement Method}%
\label{sec:value_improvement_method}

\begin{defn}
    \textbf{Policy Improvement Method}
    \begin{enumerate}
        \item Step 1: Arbitrary state value: $ V_0 $;
        \item Step 2: $ V_n = \max_a [ R(\cdot, a) + \alpha P(\cdot, a) V_{n-1} ] $.
    \end{enumerate}
    \begin{proof} Here.\\
        If $ V_0 = \vec{0} $, then 
        \begin{align*}
            V_n = V^*_n =& \max_{\pi} \mathbb{E}^{\pi = \{ d_0, d_1, \ldots, d_{n-1} \}}_n 
            \{ \sum^{n-1}_{t=0} \alpha^t r(s_t, a) | s_0  \}\\
            =& \max_{\pi} \sum^{n}_{t=1} \alpha^t \prod^{t-1}_{k=0} \langle P, d_k \rangle \langle R, d_t \rangle \\
        \end{align*}
        It can be proofed by induction, but here is intuitive description:
        \[
            V_3 =& \langle R, d_0 \rangle + \alpha \langle P, d_0 \rangle
            ( \langle R, d_1 \rangle + \alpha \langle P, d_1 \rangle \langle R, d_2 \rangle )
        \]
        The process can be saw as that: $ V_1 $ gets optimal decision $ d^*_2 $, then $ V_2 $ gets optimal decision $ d^*_1 $, 
        and $ V_3 $ gets optimal decision $ d^*_0 $, therefore, $ V_3 = V^*_3 $.
        
        If $ | r(s_t, a_t) | \le B $, then
        \[
            | V^* - V_n | \preceq \left| \mathbb{E}^\pi [ \sum^{\infty}_{t = n+1} \alpha^t r(s_t, a_t) | s_0 ]\right|
            \preceq \alpha^{n+1} B / (1 - \alpha)
        \]
        If $ V_0 \ne \vec{0} $, then we let $ V^0_n $ denote $ V_n $ when $ V_0 = 0 $, then
        \[
            V_n = V^0_n + \alpha^n \prod^{n-1}_{k=0}\langle P, d_k \rangle V_0
        \]
        \[
            |V_n - V^0_n| = \left| \alpha^n \prod^{n-1}_{k=0}\langle P, d_k \rangle V_0 \right|
            \preceq \alpha^n \sup |V_0|\vec{e}
        \]
        Then
        \[
            n \rightarrow \infty,\quad V^0_n \rightarrow V^*, V_n \rightarrow V^0_n
        \]
    \end{proof}
\end{defn}

\end{document}
