\documentclass[a4paper]{article}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}

% argmin, argmax.
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% newtheorem
\newtheorem{assumption}{Assumption}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{claim}{Claim}
\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\title{Policy Gradient}
\author{Peng Lingwei}
\begin{document}

\maketitle
\bibliographystyle{plain}
\bibliography{policy_gradient}
\tableofcontents
\newpage

\section{Stochastic Policy Gradient~\cite{sutton2000policy}}%
\label{sec:policy_gradient_for_random_policy}

\subsection{Sutton's proof (I have changed a little bit.)}%
\label{sub:sutton_s_proof}

\begin{definition}
    \textbf{(Average policy value).}
    \begin{equation}
        J (\pi) = \lim_{n \to \infty}  \frac{1}{n} \mathbb{E} \{ r_1 + r_2 + \cdots + r_n | \pi \}
        = {(d^\pi)}^T \cdot \langle \pi, R^a_s \rangle
    \end{equation}
    where $ d^\pi(s) = \lim_{n \to \infty} P \{ s_t = s | s_0, \pi \} $.
    Corresponding $ Q $ function is defined as
    \begin{equation}
        Q^{\pi}(s, a) = \sum^{\infty}_{t=1} \mathbb{E} \{ r_t - J(\pi) | s_0 = s, a_0 = a, \pi \},
        \forall s \in S, a \in A
    \end{equation}
\end{definition}

\begin{definition}
    \textbf{(Discounted policy value).}
    \begin{equation}
        J (\pi) = \sum^{}_{s_0} p_0(s_0) \mathbb{E} \left\{ \sum^{\infty}_{t=1} \gamma^{t-1} r_t \arrowvert s_0, \pi \right\}
        \quad and \quad
        Q^\pi(s,a) = \mathbb{E} \left\{ \sum^{\infty}_{k=1} \gamma^{k-1} r_{t+k} \arrowvert s_t = s, a_t = a, \pi \right\}
    \end{equation}
    then, $ d^\pi = \sum^{}_{s_0} p_0(s_0) \sum^{\infty}_{t=0} \gamma^t \Pr \left\{ s_t = s | s_0, \pi \right\} $,
    where $ p_0(s) $ is an initial state distribution.
\end{definition}

\begin{theorem}
    \textbf{(Stochastic Policy Gradient).}
    \begin{equation}
        \frac{\partial J}{\partial \theta} =  
        \sum^{}_{s} d^\pi(s) \sum^{}_{a} \frac{\partial \pi(s, a)}{\partial \theta} Q^\pi(s, a)
    \end{equation}
    \begin{proof}
    Step1: For average-reward formulation:
        \begin{align*}
            \frac{\partial{V^\pi(s)}}{\partial{\theta}} 
            =& \frac{\partial{}}{\partial{\theta}} \sum^{}_{a} \pi(s,a) Q^\pi(s,a), \quad \forall s \in S \\
            =& \sum^{}_{a} \frac{\partial{\pi(s,a)}}{\partial{\theta}} Q^\pi(s,a) + \pi(s,a) \frac{\partial{Q^\pi(s,a)}}{\partial{\theta}} \\
            =& \sum^{}_{a} \left[ \frac{\partial{\pi(s, a)}}{\partial{\theta}} Q^\pi(s,a)  + \pi(s,a) \frac{\partial{}}{\partial{\theta}} \left[ R^a_s - J(\pi) + \sum^{}_{s'} P^a_{ss'} V^\pi(s') \right] \right] \\
            =& \sum^{}_{a} \left[ \frac{\partial{\pi(s, a)}}{\partial{\theta}} Q^\pi(s,a)  + \pi(s,a) \left[ -\frac{\partial{J}}{\partial{\theta}} + \sum^{}_{s'} P^a_{ss'} \frac{\partial{V^\pi(s')}}{\partial{\theta}}  \right] \right]\\
            \frac{\partial{J}}{\partial{\theta}} 
            =& \sum^{}_{a} \left[ \frac{\partial{\pi(s, a)}}{\partial{\theta}} Q^\pi(s,a)  + \pi(s,a) \left[\sum^{}_{s'} P^a_{ss'} \frac{\partial{V^\pi(s')}}{\partial{\theta}}  \right] \right] - \frac{\partial{V^\pi(s)}}{\partial{\theta}} \\
            \sum^{}_{s} d^\pi(s) \frac{\partial{\rho}}{\partial{\theta}} 
            =& \sum^{}_{s} d^\pi(s) \sum^{}_{a} \frac{\partial{\pi(s,a)}}{\partial{\theta}} Q^\pi(s,a)
            + \sum^{}_{s} d^\pi(s) \sum^{}_{a} \pi(s,a) \sum^{}_{s'} P^a_{ss'} \frac{\partial{V^\pi(s')}}{\partial{\theta}}  \\
             &- \sum^{}_{s} d^\pi(s) \frac{\partial{V^\pi(s)}}{\partial{\theta}} \\
             \frac{\partial{J}}{\partial{\theta}} 
            =& \sum^{}_{s} d^\pi(s) \sum^{}_{a} \frac{\partial{\pi(s,a)}}{\partial{\theta}} Q^\pi(s,a)
            + \sum^{}_{s'} d^\pi(s') \frac{\partial{V^\pi(s')}}{\partial{\theta}}
            - \sum^{}_{s} d^\pi(s) \frac{\partial{V^\pi(s)}}{\partial{\theta}}\\
            =& \sum^{}_{s} d^\pi(s) \sum^{}_{a} \frac{\partial{\pi(s,a)}}{\partial{\theta}} Q^\pi(s,a)
        \end{align*}
    Step2: For discounted policy value.
        \begin{align*}
            \frac{\partial{V^\pi(s)}}{\partial{\theta}}
            =& \frac{\partial{}}{\partial{\theta}} \sum^{}_{a} \pi(s,a) Q^\pi(s,a), \quad \forall s \in S\\
            =& \sum^{}_{a} \left[ \frac{\partial{\pi(s,a)}}{\partial{\theta}} Q^\pi(s,a) + \pi(s,a) \frac{\partial{Q^\pi(s,a)}}{\partial{\theta}}  \right]\\
            =& \sum^{}_{a} \left[ \frac{\partial{\pi(s,a)}}{\partial{\theta}} Q^\pi(s,a) + \pi(s,a) \frac{\partial{}}{\partial{\theta}} \left[ R^a_{ss'} + \sum^{}_{s'} \gamma P^a_{ss'} V^\pi(s') \right]  \right]\\
            =& \sum^{}_{a} \left[ \frac{\partial{\pi(s,a)}}{\partial{\theta}} Q^\pi(s,a) + \pi(s,a) \left[ \sum^{}_{s'} \gamma P^a_{ss'} \frac{\partial{}}{\partial{\theta}} V^\pi(s') \right]  \right]\\
             &\vdots\\
            =& \sum^{}_{s'} \sum^{\infty}_{k=0} \gamma^k \Pr(s\rightarrow s', k, \pi) \sum^{}_{a} \frac{\partial{\pi(s',a)}}{\partial{\theta}} Q^\pi(s',a)\\
            \frac{\partial{J}}{\partial{\theta}} 
            =& \sum^{}_{s_0} p_0(s_0)\frac{\partial{}}{\partial{\theta}} \mathbb{E} \left\{ \sum^{\infty}_{t=1} \gamma^{t-1} r_t | s_0, \pi \right\}
            = \sum^{}_{s_0} p_0(s_0)\frac{\partial{V^\pi(s_0)}}{\partial{\theta}} \\
            =& \sum^{}_{s} \sum^{}_{s_0} p_0(s_0) \sum^{\infty}_{k=0} \gamma^k  \Pr \left\{ s_0 \rightarrow s, k, \pi \right\}
            \sum^{}_{a} \frac{\partial{\pi(s,a)}}{\partial{\theta}} Q^\pi(s,a)\\
            =& \sum^{}_{s} d^\pi(s) \sum^{}_{a} \frac{\partial{\pi(s,a)}}{\partial{\theta}} Q^\pi(s,a)
        \end{align*}
    \end{proof}
\end{theorem}

\subsection{Revised}%
\label{sub:revised}

\begin{enumerate}
    \item Initial state distribution with density $ p_0(s) $.
    \item Stationary transition dynamics distribution with conditional density: $ p(s _{t+1} | s_t, a) $.
    \item The discounted reward: $ r^\gamma_t = \sum^{\infty}_{k=t} \gamma ^{k-t} r(s_k, a_k) $.
    \item The state expected total discounted reward: $ V^\pi(s) = \mathbb{E} [ r^\gamma_0 | S_0 = s; \pi ] $ 
    \item The state-action expected total discounted reward:
        $ Q^\pi(s, a) = \mathbb{E} [ r^\gamma_0 | S_0 = s, A_0 = a; \pi] $ 
    \item Discounted state distribution $ \rho^\pi(s) = \int_S \sum^{\infty}_{t=0} \gamma^t p_0(s_0) p(s_0 \rightarrow s, t, \pi) ds_0 $ 
    \item The performance of policy:
        \begin{align*}
            J(\pi) =& \int_S \rho^\pi(s) \int_A \pi_\theta(s,a) r(s,a) da ds \\
            =& \mathbb{E}_{s\sim\rho^\pi, a\sim\pi_\theta} [r(s, a)]
        \end{align*}
    \item Stochastic Policy Gradient Theorem:
        \begin{align*}
            \nabla_\theta J(\pi_\theta) =& \int_S \rho^\pi(s) \int_A \nabla_\theta \pi_\theta(a | s) Q^\pi(s,a) da ds \\
            =& \mathbb{E}_{s\sim \rho^\pi, a \sim \pi_\theta}
            [ \nabla_\theta \log \pi_\theta (a | s) Q^\pi(s, a) ]
        \end{align*}
\end{enumerate}

\subsection{Stochastic Actor-Critic Algorithms}%
\label{sub:stochastic_actor_critic_algorithms}

We let $ Q^w(s,a) $ to approximate $ Q^\pi(s, a) $. 

\begin{theorem}
    If $ Q^w(s,a) = {(\nabla_\theta \log \pi_\theta(a|s))}^T w $
    and $ w = \arg\min_w \mathbb{E} _{s\sim \rho^\pi, a \sim \pi_\theta} \left[ {\left( Q^w(s,a) - Q^\pi(s,a) \right)}^2 \right] $,
    then
    \[
        \nabla_\theta J(\pi_\theta) = \mathbb{E}_{s\sim \rho^\pi, a \sim \pi_\theta}
        [\nabla_\theta \log \pi_\theta(a|s) Q^w(s,a)]
    \]
    \begin{proof}
        \begin{align*}
            0 =& \nabla_w \mathbb{E} _{s\sim \rho^\pi, a \sim \pi_\theta} \left[ {\left( Q^w(s,a) - Q^\pi(s,a) \right)}^2 \right] \\
            =& \mathbb{E} _{s\sim \rho^\pi, a \sim \pi_\theta} \left[ {\left( Q^w(s,a) - Q^\pi(s,a) \right)} \nabla_w Q^w(w,a) \right] \\
            =& \mathbb{E} _{s\sim \rho^\pi, a \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) {\left( Q^w(s,a) - Q^\pi(s,a) \right)} \right] \\
            \nabla_\theta J(\pi_\theta) 
            =& \mathbb{E}_{s\sim \rho^\pi, a \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(a|s) Q^w(s,a)]
        \end{align*}
    \end{proof}
\end{theorem}

\begin{definition}
    \textbf{(Stochastic Actor-Critic algorithm)}.
    \begin{enumerate}
        \item \textbf{critic}: $ w_{critic} $, such that $ \mathbb{E}_{s\sim \rho^{\pi_{\theta_k}}, a \sim \pi_{\theta_k}} \left[ \nabla_{\theta} \log \pi_{\theta_k} (a|s) {\left( Q^{w_{critic}}(s,a) - Q^{\pi_{\theta_k}} (s,a) \right)} \right]=0 $.
        \item \textbf{actor}: $ \theta_{k+1} = \theta_k + \alpha_k  \mathbb{E}_{s\sim \rho^{\pi_{\theta_k}}, a \sim \pi_{\theta_k}} [\nabla_\theta \log \pi_{\theta_k}(a|s) Q^{w_{critic}}(s,a)]$ 
    \end{enumerate}
\end{definition}

\section{Deterministic Policy Gradient~\cite{silver2014deterministic}}%
\label{sec:deterministic_policy_gradient}

\subsection{Basic Proof}%
\label{sub:basic_proof}

\textbf{Conditions}
\begin{enumerate}
    \item $ p(s' | s, a), \nabla_a p(s' | s, a), \mu_\theta (s), \nabla_\theta \mu_\theta(s), r(s, a), \nabla_a r(s,a), p_0(s)$ are continuous in all parameters and variables $ s, a, s'$ and $ x $.
    \item $ \exists b, L $, $ \sup_s p_0(s) < b $, $ \sup_{a, s, s'} p(s' | s,a) < b $, $ \sup_{a,s} r(s,a) < b $,
        $ \sup_{a,s,s'} \Arrowvert \nabla_a p(s'|s, a) \Arrowvert < L $,\ 
        and $ \sup_{a,s} \Arrowvert \nabla_a r(s,a) \Arrowvert < L $.
\end{enumerate}
\begin{definition}
    (Greedy Policy, or Deterministic Policy)
    \[
        \mu_\theta(s) = \arg\max_a Q^{\mu_\theta}(s, a).
    \]
\end{definition}
\begin{definition}
    \textbf{(Deterministic discounted policy value).}
    \[
        J(\mu_\theta) = \int_S \rho^{\mu_\theta}(s) r(s, \mu_\theta(s)) ds
        = \mathbb{E}_{s\sim\rho^{\mu_\theta}} [r(s, \mu_\theta(s))]
    \]
    \[
        Q^{\mu_\theta} (s, a)
        = \mathbb{E} \left\{ \sum^{\infty}_{k=1} \gamma^{k-1} r_{t+k} | s_t = s, a_t = a, \pi \right\}
    \]
    \[
        \rho ^{\mu_\theta}(s_0) = \int_S \sum^{\infty}_{t=0} \gamma^t p_0(s_0) p(s_0 \rightarrow s, t, \mu_\theta) d s_0
    \]
    
    
\end{definition}

\begin{theorem}
    \textbf{(Deterministic Policy Gradient Theorem)}.
    If preceeding conditions are satisfied, and $ \nabla_\theta \mu_\theta(s) $ and $ \nabla_a Q^{\mu_\theta}(s, a) $ exist,
    and that the deterministic policy gradient exists. Then,
    \begin{equation}
        \begin{aligned}
            \nabla_\theta J(\mu_\theta) 
            =& \int_S \rho^{\mu_\theta}_\theta(s) \nabla_\theta \mu_\theta(s) \nabla_a Q ^{\mu_\theta} (s, a)|_{a=\mu_\theta} ds \\
            =& \mathbb{E}_{s \sim \rho^{\mu_\theta}}
            [ \nabla_\theta \mu_\theta(s) \nabla_a Q^{\mu_\theta} (s, a) |_{a=\mu_\theta} ]
        \end{aligned}
    \end{equation}
    \begin{proof}
        \begin{align*}
            \nabla_\theta V ^{\mu_\theta}(s)
            =& \nabla_\theta Q ^{\mu_\theta}(s, \mu_\theta(s)) \\
            =& \nabla_\theta \left( r(s, \mu_\theta(s)) + \int_S \gamma p(s'|s, \mu_\theta(s)) V^{\mu_\theta}(s') ds' \right) \\
            =& \nabla_\theta \mu_\theta(s) \nabla_a r(s,a) |_{a=\mu_\theta(s)}
                + \nabla_\theta \int_S \gamma p(s'|s, \mu_\theta(s)) V^{\mu_\theta}(s') ds'\\
            =& \nabla_\theta \mu_\theta(s) \nabla_a r(s,a)|_{a=\mu_\theta(s)} \\
             &+ \int_S \gamma \left(
                p(s'|s,\mu_\theta(s)) \nabla_\theta V^{\mu_\theta}(s')
                + \nabla_\theta \mu_\theta(s) \nabla_a p(s'|s, a)|_{a=\mu_\theta(s)} V^{\mu_\theta}(s')
             \right) ds'\\
            =& \nabla_\theta \mu_\theta(s) \nabla_a \left( r(s,a) + \int_S \gamma p(s'|s,a) V^{\mu_\theta} (s') ds' \right)|_{a=\mu_\theta(s)} \\
             &+ \int_S \gamma p(s'|s,\mu_\theta(s)) \nabla_\theta V^{\mu_\theta} (s') ds' \\
            =& \nabla_\theta \mu_\theta(s) \nabla_a Q^{\mu_\theta} (s, a) |_{a=\mu_\theta (s)}
            + \int_S \gamma p(s\rightarrow s', 1, \mu_\theta) \nabla_\theta V^{\mu_\theta} (s') ds' \\
             &\vdots\\
            =& \int_S \sum^{\infty}_{t=0} \gamma^t p(s \rightarrow s', t, \mu_\theta) \nabla_\theta \mu_\theta(s') \nabla_a Q^{\mu_\theta} (s', a) |_{a=\mu_\theta(s')} ds'\\
            \nabla_\theta J(\mu_\theta)
            =& \nabla_\theta \int_S p_0(s) V ^{\mu_\theta}(s) ds \\
            =& \int_S p_0(s) \nabla_\theta V ^{\mu_\theta}(s) ds \\
            =& \int_S \int_S \sum^{\infty}_{t=0} \gamma^t p_0(s) p(s \rightarrow s', t, \mu_\theta) \nabla_\theta \mu_\theta(s') \nabla_a Q^{\mu_\theta} (s', a) |_{a=\mu_\theta(s')} ds' ds\\
            =& \int_S \int_S \sum^{\infty}_{t=0} \gamma^t p_0(s_0) p(s_0 \rightarrow s, t, \mu_\theta) ds_0 \nabla_\theta \mu_\theta(s)\nabla_a Q^{\mu_\theta}(s,a)|_{a=\mu_\theta(s)} ds\\
            =& \int_S \rho ^{\mu_\theta}(s) \nabla_\theta \mu_\theta(s) \nabla_a Q^{\mu_\theta}(s, a) |_{a=\mu_\theta(s)} ds
        \end{align*}
    \end{proof}
\end{theorem}

\subsection{The Relationship Between Stochastic and Deterministic Policy Gradient (Unfinished)}%
\label{sub:the_relationship_between_stochastic_and_deterministic_policy_gradient}

\begin{theorem}
    Deterministic policy gradient is a special case of the stochastic policy gradient.
\end{theorem}

\end{document}
